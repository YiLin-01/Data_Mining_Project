{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Annotations\n",
    "- annotations.csv lack with GENDER and other attributes\n",
    "- annotations.csv includes GENDER but lost other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import scipy.io\n",
    "from scipy.io import loadmat, savemat\n",
    "from maskable_list import MaskableList\n",
    "\n",
    "\n",
    "ann_df = pd.read_csv('/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/annotations.csv')\n",
    "ann_new_df = pd.read_csv('/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/annontations_New.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1206, 24)\n",
      "(1206, 573)\n",
      "(1206, 596)\n"
     ]
    }
   ],
   "source": [
    "print(ann_df.shape)\n",
    "print(ann_new_df.shape)\n",
    "# pd.concat([ann_df, ann_new_df], axis=1)\n",
    "ann_combined = pd.merge(ann_df, ann_new_df, on='Subject', how='inner')\n",
    "print(ann_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the center coordinate matrix (i.e. XXROI_center.mat) fitler the subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of subjects should be 1040\n",
      "(1040, 596)\n"
     ]
    }
   ],
   "source": [
    "ROI_center_132 = scipy.io.loadmat('/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/132ROI_center.mat')\n",
    "# ROI_center_82 = scipy.io.loadmat('/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/82ROI_center.mat')\n",
    "\n",
    "subject_ids = [] \n",
    "# print(ROI_center_132['ROI_center'][0][1])\n",
    "\n",
    "for subject in ROI_center_132['ROI_center']:\n",
    "    if subject[1].shape[0] > 0: # subject[1] is ndarray(132, 3)\n",
    "        subject_ids.append(int(subject[0][0])) # The subject id, type = ndarray -> int\n",
    "        \n",
    "print(\"The number of subjects should be\", len(subject_ids))\n",
    "ann_filtered = ann_combined[ann_combined['Subject'].isin(subject_ids)]\n",
    "print(ann_filtered.shape)\n",
    "ann_filtered.to_csv('/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/filtered_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "ann_filtered['Gender'].unique()\n",
    "print(len(ann_filtered['Gender']))\n",
    "print(len(ann_filtered['Subject']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the brain data\n",
    "- functional network: derivedresting-state fMRI (1063 subjects)\n",
    "    - Normalized weighted adjacency matrix for each subject (.mat)\n",
    "    - Filename is the subject ID\n",
    "- structural network: derived from diffusion MRI (1063 subjects)\n",
    "    - Adjacency matrix for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = '/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/functional_network_132_0'\n",
    "# replace_nan_in_mat(directory)\n",
    "# adj_dict = loadmat(f'{directory}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adj_label_HCP(data_dir, label_dir, modality='dti'):\n",
    "    \"\"\"\n",
    "    load adjacency matrix from raw matlab (G, N, N) & load the label from csv file (G,).\n",
    "    We need to make them a correct pair!\n",
    "\n",
    "    Args:\n",
    "        data_dir: root dir to Ã¥save the dataset\n",
    "        dataset: name of the dataset\n",
    "        modality: 'dti' by default (structural MRI, SMRI) or 'func' (FMRI)\n",
    "        is_dup: Whether the label file has duplicated subject\n",
    "\n",
    "    returns:\n",
    "        adj: (G, N, N), G is the graph for cleaned subjects\n",
    "        y: (G,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dict of adj matrix (including all the subjects such dirty ones, duplicated ones)\n",
    "\n",
    "    # dirty currently\n",
    "    # Don't no if need to transpose the adj matrix\n",
    "    # adj = adj_dict[adj_key].transpose((2, 1, 0))  # np.narray (# dirty graphs/substances, # node, # node)\n",
    "\n",
    "    # Find the Label from the csv file\n",
    "    #label_file_name = os.path.splitext(label_dir)[0]\n",
    "    # demo_file = f'{label_file_name}' + ('_dup.csv' if Is_dup else '.csv')\n",
    "    \n",
    "    demo_df = pd.read_csv(label_dir)\n",
    "    select_cols = ['Subject', 'Race', 'Gender', 'Age']\n",
    "    demo_df = demo_df[select_cols]\n",
    "    \n",
    "    adj_list = []\n",
    "    label_list = []\n",
    "    # Get the corresponding adj matrix\n",
    "    for idx, sub in enumerate(demo_df['Subject']):\n",
    "        if not os.path.isfile(f'{data_dir}/{sub}.mat'):\n",
    "            print(f'{sub}.mat does not exist in the adj path!')\n",
    "            continue\n",
    "\n",
    "        ##dirty currently\n",
    "        ## adj = adj_dict[adj_key].transpose((2, 1, 0))  # np.narray (# dirty graphs/substances, # node, # node)\n",
    "        ## adj = extract_knn(a1, args.top_k) # when args.top_k == 0, return the original a1\n",
    "\n",
    "        ## Get the corresponding adj from adj path\n",
    "        ## adj = adj[sub_list, :, :]\n",
    "        adj_dict = loadmat(f'{data_dir}/{sub}')\n",
    "        adj_key = list(adj_dict.keys())[-1]\n",
    "        adj = torch.Tensor(adj_dict[adj_key])\n",
    "\n",
    "        # Make the negative weights positive for fmri\n",
    "        if modality == 'func':\n",
    "            adj[adj < 0] = -adj[adj < 0]\n",
    "            mask = torch.isnan(adj)\n",
    "            adj[mask] = 0\n",
    "        elif modality == 'dti':  # Min-Max normalization for smri\n",
    "            adj = (adj - adj.min()) / (adj.max() - adj.min())\n",
    "        adj_list.append(adj)\n",
    "\n",
    "        # Process labels\n",
    "        gender = demo_df.loc[idx, 'Gender']\n",
    "        label = 0 if gender == 'M' else 1\n",
    "        label_list.append(label)\n",
    "        \n",
    "    adj_tensor = torch.stack(adj_list) if adj_list else torch.empty(0)\n",
    "    y = torch.tensor(label_list, dtype=torch.long) if label_list else torch.empty(0, dtype=torch.long)\n",
    "    # y = torch.tensor(label_df).long().flatten()  # labels for all the graph (#graph)\n",
    "\n",
    "    return adj_tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150019.mat does not exist in the adj path!\n",
      "173233.mat does not exist in the adj path!\n",
      "248238.mat does not exist in the adj path!\n",
      "693461.mat does not exist in the adj path!\n",
      "995174.mat does not exist in the adj path!\n",
      "adj matrix size: torch.Size([1035, 132, 132])\n",
      "label size: torch.Size([1035])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node_features(adj, node_feature_type='identity', f_dim=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        adj: weighted adjacency matrix (G, N, N), G is the graph for cleaned subjects\n",
    "        node_feature_type: The way to generate the node features\n",
    "        f_dim: optional, not all of type will use it\n",
    "    returns:\n",
    "        x1: node feature (#graph, #node, #f_dim)\n",
    "    \"\"\"\n",
    "    n_graph = adj.shape[0]\n",
    "    n_node = adj.shape[1]\n",
    "    if f_dim is None:\n",
    "        f_dim = n_node\n",
    "\n",
    "    # construct node features X\n",
    "    if node_feature_type == 'identity':\n",
    "        x1 = torch.ones(n_graph, n_node, f_dim)\n",
    "    # elif node_feature_type == 'onehot':   #  one-hot\n",
    "    #     # (#graph, #node, #f_dim), where the feature is one hot vector, #f_dim=#nodes\n",
    "    #     # for each graph, it is a dialog matrix, so for each row (node), it is one-hot vector\n",
    "    #     x = torch.cat([torch.diag(torch.ones(n_node))] * n_graph).reshape([n_graph, n_node, -1])\n",
    "    #     x1 = x.clone()  # (#graph, #node, #f_dim=#nodes)\n",
    "\n",
    "    # elif node_feature_type == 'degree':\n",
    "    #     # use degree to represent its feature\n",
    "    #     a1b = (adj != 0).float()  # 0./1. tensor with (#graph, #node, #node)\n",
    "    #     x1 = a1b.sum(dim=2, keepdim=True)  # (#graph, #node, #f_dim=1)\n",
    "    #################################################################################\n",
    "    elif node_feature_type == 'adj':  # use the adj matrix directly\n",
    "        x1 = adj.float()  # (#graph, #node, #nodes)\n",
    "    #################################################################################\n",
    "    # elif node_feature_type == 'LDP':  # degree profile\n",
    "    #     a1b = (adj != 0).float()  # 0./1. tensor with (#graph, # node, # node)\n",
    "    #     x1 = []\n",
    "    #     for i in range(n_graph):\n",
    "    #         x1.append(LDP(nx.from_numpy_array(a1b[i].numpy())))\n",
    "\n",
    "    # elif node_feature_type == 'eigen':\n",
    "    #     _, x = LA.eig(adj.numpy())  # where is x1?\n",
    "    #     x1 = x  # we add\n",
    "\n",
    "    # elif node_feature_type == 'degree_bin':\n",
    "    #     a1b = (adj != 0).float()\n",
    "    #     x1 = binning(a1b.sum(dim=2))  # (#graph, # node, # 1)\n",
    "    #################################################################################\n",
    "    elif node_feature_type == 'gaussian':  # standard normalization (mu=0,std=1)\n",
    "        x1 = torch.normal(mean=0., std=1., size=(n_graph, n_node, f_dim))\n",
    "        # x1 = torch.randn(n_graph, n_node, f_dim) # equivalent\n",
    "    else:\n",
    "        raise Exception(\"The type to generate node features is not supported\")\n",
    "    #################################################################################\n",
    "    # elif node_feature_type == 'node2vec':\n",
    "    #     X = np.load(f'./{args.dataset_name}_{args.modality}.emb', allow_pickle=True).astype(np.float32)\n",
    "    #     x1 = torch.from_numpy(X)\n",
    "\n",
    "    x1 = torch.Tensor(x1).float()\n",
    "    return x1\n",
    "\n",
    "\n",
    "# mark the edge index in tiled n^2 vector\n",
    "def generate_edge_flag(n_node, edge_index):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        edge_index: (2, E)\n",
    "\n",
    "    returns:\n",
    "        edge_flag: (n_node*n_node, ), 1-dim, tensor. if non-zero, edge_flag is True, else False, bool Tensor\n",
    "    \"\"\"\n",
    "    edge_flag = np.full((n_node ** 2,), False)\n",
    "    n_edge = edge_index.shape[1]\n",
    "    for i in range(n_edge):\n",
    "        source = edge_index[0, i]\n",
    "        target = edge_index[1, i]\n",
    "        new_index = source * n_node + target\n",
    "        edge_flag[new_index] = True\n",
    "\n",
    "    edge_flag = torch.from_numpy(edge_flag)\n",
    "    return edge_flag\n",
    "\n",
    "\n",
    "def dense_adj_2_COO(adj):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        adj: dense adjacency matrix, (G, N, N)\n",
    "\n",
    "    returns:\n",
    "        ls_edge_index is list of the COO connectivity, ls_edge_weight is list the weight of each connected edge (scalar),\n",
    "        ls_edge_flag is the list of flattened binary adj matrix.\n",
    "        All the elements in the list is tensor\n",
    "\n",
    "        ls_edge_index: 0/1 entries, [(2, E), ...], where len is G\n",
    "        ls_edge_weight: [(E), ...], where len is G\n",
    "        ls_edge_flag: binary entries, [(N*N, ), ..], where len is G\n",
    "    \"\"\"\n",
    "    ls_edge_index = []\n",
    "    ls_edge_weight = []  # Get it from the adj matrix\n",
    "    ls_edge_flag = []\n",
    "    n_graph = adj.shape[0]\n",
    "    n_node = adj.shape[1]\n",
    "    for i in range(n_graph):  # For each graph\n",
    "        edge_index, edge_weight = dense_to_sparse(adj[i])  # (2, E), (E)\n",
    "        # print(\"I am here. {}\".format(i))\n",
    "        # input(binary, 0/1): (2, E) -> output(binary, 0/1): (N*N), still 1-dim\n",
    "        edge_flag = generate_edge_flag(n_node, edge_index)  # For each graph, flattened binary adj matrix (N*N)/flattened edge_index\n",
    "\n",
    "        ls_edge_index.append(edge_index)\n",
    "        ls_edge_weight.append(edge_weight)\n",
    "        ls_edge_flag.append(edge_flag)\n",
    "\n",
    "    return ls_edge_index, ls_edge_weight, ls_edge_flag\n",
    "\n",
    "\n",
    "def build_dataset(x, ls_edge_index, ls_edge_weight, ls_edge_flag, y, adj):\n",
    "    \"\"\"\n",
    "    Args: (all of the elements are tensor)\n",
    "        x: node feature tensor (G, N, F)\n",
    "        ls_edge_index: 0/1 entries, [(2, E), ...], where len is G\n",
    "        ls_edge_weight: [(E), ...], where len is G\n",
    "        ls_edge_flag: 0/1 entries, [(N*N, ), ..], where len is G, bool Tensor\n",
    "        y: (G,), labels of all the graphs\n",
    "        adj: (G, N, N), G is the graph for cleaned subjects\n",
    "    returns:\n",
    "\n",
    "    \"\"\"\n",
    "    data_list = MaskableList([])\n",
    "    n_graph = y.shape[0]\n",
    "    for idx in range(n_graph):\n",
    "        data = Data(x=x[idx], edge_index=ls_edge_index[idx],\n",
    "                    edge_weight=ls_edge_weight[idx], edge_attr=ls_edge_weight[idx],\n",
    "                    edge_flag=ls_edge_flag[idx], y=y[idx], adj=adj[idx])\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data list for dataset\n",
      "Done.\n",
      "The total time for data loading is 0.002062666416168213 minutes\n",
      "\n",
      "torch.Size([1035, 132, 132])\n",
      "torch.Size([1035])\n",
      "1035\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# sequence for data loading\n",
    "data_dir = '/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/functional_network_132_0'\n",
    "label_dir = '/Users/lihanzhao/Desktop/W&M/S24/BrainGNN/HCP_Data/filtered_annotations.csv'\n",
    "f_dim = None\n",
    "node_feature_type = 'gaussian'\n",
    "print(\"Get adj & y\")\n",
    "adj, y = load_adj_label_HCP(data_dir, label_dir, modality='func')\n",
    "print('adj matrix size:', adj.shape)\n",
    "print('label size:', y.shape)\n",
    "print()\n",
    "\n",
    "print(\"get node features\")\n",
    "x = generate_node_features(adj=adj, node_feature_type=node_feature_type, f_dim=f_dim)\n",
    "\n",
    "print(\"get edge characters in COO format\")\n",
    "ls_edge_index, ls_edge_weight, ls_edge_flag = dense_adj_2_COO(adj)\n",
    "\n",
    "print(\"getting data list for dataset\")\n",
    "data_list = build_dataset(x, ls_edge_index, ls_edge_weight, ls_edge_flag, y, adj)\n",
    "print(\"Done.\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"The total time for data loading is {} minutes\".format((end-start)/60))\n",
    "print()\n",
    "print(adj.shape)\n",
    "print(y.shape)\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braingnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
